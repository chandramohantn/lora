{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "74a862c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn.utils.parametrize as parametrize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b0e29d",
   "metadata": {},
   "source": [
    "#### Make torch deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c373026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10e3c6550>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3851017",
   "metadata": {},
   "source": [
    "### Load MNSIT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85def474",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "DATA_DIR = './data'\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 10\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff446f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "train_dataset = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=transform)\n",
    "validation_dataset = datasets.MNIST(root=DATA_DIR, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd96be",
   "metadata": {},
   "source": [
    "### Creating a simple Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4f798185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d57f444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTClassifier().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2188cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, validation_loader):\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(validation_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = validation_loss / len(validation_loader)\n",
    "    print(f\"Epoch: {epoch+1} Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "50844315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, train_loader):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch_index, data in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{EPOCHS}\")):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if (batch_index + 1) % 1000 == 0:\n",
    "            print(f\"Epoch: {epoch+1}, Batch: {batch_index+1}, Loss: {epoch_loss:.4f}\")\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2147f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, validation_loader):\n",
    "    stop_counter = 0\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_one_epoch(epoch, train_loader)\n",
    "        val_loss, val_accuracy = validation(epoch, validation_loader)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            stop_counter = 0\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Model saved with validation loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            stop_counter += 1\n",
    "            if stop_counter >= EARLY_STOPPING_PATIENCE:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e21de6",
   "metadata": {},
   "source": [
    "### Train the MNIST classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a85010b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50:  54%|█████▍    | 1008/1875 [00:17<00:15, 56.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 1000, Loss: 195.1587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50: 100%|██████████| 1875/1875 [00:33<00:00, 56.51it/s]\n",
      "Validation Epoch 1: 100%|██████████| 313/313 [00:01<00:00, 157.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Validation Loss: 0.0442, Accuracy: 98.57%\n",
      "Epoch 1/50, Train Loss: 246.8885, Validation Loss: 0.0442, Validation Accuracy: 98.57%\n",
      "Model saved with validation loss: 0.0442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/50:  54%|█████▍    | 1008/1875 [00:18<00:15, 57.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Batch: 1000, Loss: 42.6863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/50: 100%|██████████| 1875/1875 [00:33<00:00, 56.41it/s]\n",
      "Validation Epoch 2: 100%|██████████| 313/313 [00:01<00:00, 158.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Validation Loss: 0.0366, Accuracy: 98.75%\n",
      "Epoch 2/50, Train Loss: 76.3925, Validation Loss: 0.0366, Validation Accuracy: 98.75%\n",
      "Model saved with validation loss: 0.0366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/50:  54%|█████▍    | 1008/1875 [00:17<00:15, 56.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Batch: 1000, Loss: 28.9526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/50: 100%|██████████| 1875/1875 [00:33<00:00, 55.97it/s]\n",
      "Validation Epoch 3: 100%|██████████| 313/313 [00:02<00:00, 154.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Validation Loss: 0.0319, Accuracy: 98.92%\n",
      "Epoch 3/50, Train Loss: 53.0841, Validation Loss: 0.0319, Validation Accuracy: 98.92%\n",
      "Model saved with validation loss: 0.0319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/50:  54%|█████▍    | 1008/1875 [00:18<00:15, 56.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Batch: 1000, Loss: 18.8826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/50: 100%|██████████| 1875/1875 [00:33<00:00, 55.42it/s]\n",
      "Validation Epoch 4: 100%|██████████| 313/313 [00:01<00:00, 158.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Validation Loss: 0.0448, Accuracy: 98.64%\n",
      "Epoch 4/50, Train Loss: 38.9755, Validation Loss: 0.0448, Validation Accuracy: 98.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/50:  54%|█████▍    | 1008/1875 [00:18<00:15, 55.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Batch: 1000, Loss: 14.4855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/50: 100%|██████████| 1875/1875 [00:33<00:00, 55.41it/s]\n",
      "Validation Epoch 5: 100%|██████████| 313/313 [00:02<00:00, 146.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Validation Loss: 0.0326, Accuracy: 98.99%\n",
      "Epoch 5/50, Train Loss: 29.1234, Validation Loss: 0.0326, Validation Accuracy: 98.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/50:  54%|█████▍    | 1008/1875 [00:17<00:15, 56.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Batch: 1000, Loss: 11.7264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/50: 100%|██████████| 1875/1875 [00:33<00:00, 56.13it/s]\n",
      "Validation Epoch 6: 100%|██████████| 313/313 [00:01<00:00, 160.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Validation Loss: 0.0349, Accuracy: 98.96%\n",
      "Epoch 6/50, Train Loss: 21.9345, Validation Loss: 0.0349, Validation Accuracy: 98.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/50:  54%|█████▍    | 1008/1875 [00:18<00:15, 56.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Batch: 1000, Loss: 7.7661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/50: 100%|██████████| 1875/1875 [00:33<00:00, 55.66it/s]\n",
      "Validation Epoch 7: 100%|██████████| 313/313 [00:02<00:00, 153.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Validation Loss: 0.0393, Accuracy: 98.95%\n",
      "Epoch 7/50, Train Loss: 19.7054, Validation Loss: 0.0393, Validation Accuracy: 98.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/50:  54%|█████▍    | 1010/1875 [00:19<00:15, 54.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Batch: 1000, Loss: 6.9122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/50: 100%|██████████| 1875/1875 [00:34<00:00, 54.25it/s]\n",
      "Validation Epoch 8: 100%|██████████| 313/313 [00:01<00:00, 160.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Validation Loss: 0.0377, Accuracy: 99.08%\n",
      "Epoch 8/50, Train Loss: 15.8468, Validation Loss: 0.0377, Validation Accuracy: 99.08%\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7663a73",
   "metadata": {},
   "source": [
    "### Keep a copy of the trained model (compare after performing LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cff4f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_weights = {}\n",
    "for name, param in model.named_parameters():\n",
    "    original_weights[name] = param.clone().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c287a729",
   "metadata": {},
   "source": [
    "### Check performance of the trained model all digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e3dacb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 313/313 [00:02<00:00, 149.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0319, Accuracy: 98.92%\n",
      "Digit 0: 99.59% Incorrect: 4 Total: 980\n",
      "Digit 1: 99.65% Incorrect: 4 Total: 1135\n",
      "Digit 2: 99.42% Incorrect: 6 Total: 1032\n",
      "Digit 3: 99.70% Incorrect: 3 Total: 1010\n",
      "Digit 4: 98.57% Incorrect: 14 Total: 982\n",
      "Digit 5: 98.32% Incorrect: 15 Total: 892\n",
      "Digit 6: 98.64% Incorrect: 13 Total: 958\n",
      "Digit 7: 98.93% Incorrect: 11 Total: 1028\n",
      "Digit 8: 98.05% Incorrect: 19 Total: 974\n",
      "Digit 9: 98.12% Incorrect: 19 Total: 1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.031945871058609375, 98.92)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test():\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct, total = 0, 0\n",
    "    digit_count = {i: 0 for i in range(NUM_CLASSES)}\n",
    "    digit_correct = {i: 0 for i in range(NUM_CLASSES)}\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(validation_loader, desc=\"Testing\"):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            for i in range(len(labels)):\n",
    "                digit_count[labels[i].item()] += 1\n",
    "                if predicted[i] == labels[i]:\n",
    "                    digit_correct[labels[i].item()] += 1\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = test_loss / len(validation_loader)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    for digit in range(NUM_CLASSES):\n",
    "        if digit_count[digit] > 0:\n",
    "            digit_accuracy = 100 * digit_correct[digit] / digit_count[digit]\n",
    "        else:\n",
    "            digit_accuracy = 0\n",
    "        print(f\"Digit {digit}: {digit_accuracy:.2f}% Incorrect: {digit_count[digit] - digit_correct[digit]} Total: {digit_count[digit]}\")\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5064f24d",
   "metadata": {},
   "source": [
    "### Number of parameters in the MNIST classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "276dc4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: 320 parameters\n",
      "Layer 2: 18496 parameters\n",
      "Layer 3: 401536 parameters\n",
      "Layer 4: 1290 parameters\n",
      "Total parameters in the model: 421642\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for index, layer in enumerate([model.conv1, model.conv2, model.fc1, model.fc2]):\n",
    "    num_params = sum(p.numel() for p in layer.parameters())\n",
    "    total_params += num_params\n",
    "    print(f\"Layer {index+1}: {num_params} parameters\")\n",
    "print(f\"Total parameters in the model: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2571d7",
   "metadata": {},
   "source": [
    "### LoRA Parameterization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d6d7b",
   "metadata": {},
   "source": [
    "##### Use a gaussian initialization for A and zero for B, so delta W = BA is zero at the begining of training. We then scale delta Wx by alpha/r, where alpha is a constant in r. When optimizing with Adam, tuning alpha is roughly the same as tuning the learning rate if we scale the initialization appropriately. As a result, we simply set alpha to the first r we try and do not tune it. This scaling helps to reduce the need to retune hyperparameters when we vary r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e510a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRAParameterization(nn.Module):\n",
    "    def __init__(self, dim_input, dim_output, rank=1, alpha=1, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.lora_A = nn.Parameter(torch.zeros(rank, dim_output).to(device))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(dim_input, rank).to(device))\n",
    "        nn.init.normal_(self.lora_A, mean=0, std=1)\n",
    "        self.scale = alpha / rank\n",
    "        self.enabled = True\n",
    "\n",
    "    def forward(self, original_weights):\n",
    "        if self.enabled:\n",
    "            return original_weights + (self.lora_B @ self.lora_A).view(original_weights.shape) * self.scale\n",
    "        else:\n",
    "            return original_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116d31d",
   "metadata": {},
   "source": [
    "#### Create a LoRA parameterization instance for a given linear layer by inspecting its weight shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2d920de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n",
    "    features_in, features_out = layer.weight.shape\n",
    "    lora = LoRAParameterization(features_in, features_out, rank=rank, alpha=lora_alpha, device=device)\n",
    "    return lora\n",
    "\n",
    "RANK = 4\n",
    "parametrize.register_parametrization(\n",
    "    model.fc1, \"weight\", linear_layer_parameterization(model.fc1, device=DEVICE, rank=RANK)\n",
    ")\n",
    "\n",
    "parametrize.register_parametrization(\n",
    "    model.fc2, \"weight\", linear_layer_parameterization(model.fc2, device=DEVICE, rank=RANK)\n",
    ")\n",
    "\n",
    "def enable_disable_lora(enabled=True):\n",
    "    for layer in [model.fc1, model.fc2]:\n",
    "        layer.parametrizations[\"weight\"][0].enabled = enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f980a620",
   "metadata": {},
   "source": [
    "#### Number of parameters added by LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9d222452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: W: torch.Size([128, 3136]) + B: torch.Size([128]) + A: torch.Size([4, 3136]) + B: torch.Size([128, 4])\n",
      "Layer 2: W: torch.Size([10, 128]) + B: torch.Size([10]) + A: torch.Size([4, 128]) + B: torch.Size([10, 4])\n",
      "Total parameters in LoRA: 13608\n",
      "Total parameters in non-LoRA: 402826\n"
     ]
    }
   ],
   "source": [
    "total_parameters_lora = 0\n",
    "total_parameters_non_lora = 0\n",
    "for index, layer in enumerate([model.fc1, model.fc2]):\n",
    "    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.numel() + layer.parametrizations[\"weight\"][0].lora_B.numel()\n",
    "    total_parameters_non_lora += layer.weight.numel() + layer.bias.numel()\n",
    "    print(f\"Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + A: {layer.parametrizations['weight'][0].lora_A.shape} + B: {layer.parametrizations['weight'][0].lora_B.shape}\")\n",
    "\n",
    "print(f\"Total parameters in LoRA: {total_parameters_lora}\")\n",
    "print(f\"Total parameters in non-LoRA: {total_parameters_non_lora}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea842c0f",
   "metadata": {},
   "source": [
    "#### Freeze all linear layer parameters and fine tune the LoRA parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04a428fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing non-lora parameters: conv1.weight\n",
      "Freezing non-lora parameters: conv1.bias\n",
      "Freezing non-lora parameters: conv2.weight\n",
      "Freezing non-lora parameters: conv2.bias\n",
      "Freezing non-lora parameters: fc1.bias\n",
      "Freezing non-lora parameters: fc1.parametrizations.weight.original\n",
      "Freezing non-lora parameters: fc2.bias\n",
      "Freezing non-lora parameters: fc2.parametrizations.weight.original\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'lora' not in name:\n",
    "        print(f\"Freezing non-lora parameters: {name}\")\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2703cf2",
   "metadata": {},
   "source": [
    "#### Load MNIST data and retrain the least accurate digit again using LoRA params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "faf81059",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_lora_train = datasets.MNIST(root=DATA_DIR, train=True, download=True, transform=transform)\n",
    "exclude_indices = mnist_lora_train.targets == 8\n",
    "mnist_lora_train.data = train_dataset.data[exclude_indices]\n",
    "mnist_lora_train.targets = train_dataset.targets[exclude_indices]\n",
    "mnist_lora_train_loader = DataLoader(mnist_lora_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "mnist_lora_validation = datasets.MNIST(root=DATA_DIR, train=False, download=True, transform=transform)\n",
    "exclude_indices = mnist_lora_validation.targets == 8\n",
    "mnist_lora_validation.data = validation_dataset.data[exclude_indices]\n",
    "mnist_lora_validation.targets = validation_dataset.targets[exclude_indices]\n",
    "mnist_lora_validation_loader = DataLoader(mnist_lora_validation, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c04c48af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50: 100%|██████████| 183/183 [00:01<00:00, 114.10it/s]\n",
      "Validation Epoch 1: 100%|██████████| 31/31 [00:00<00:00, 132.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Validation Loss: 0.0596, Accuracy: 98.05%\n",
      "Epoch 1/50, Train Loss: 5.3257, Validation Loss: 0.0596, Validation Accuracy: 98.05%\n",
      "Model saved with validation loss: 0.0596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/50: 100%|██████████| 183/183 [00:01<00:00, 117.30it/s]\n",
      "Validation Epoch 2: 100%|██████████| 31/31 [00:00<00:00, 127.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Validation Loss: 0.0596, Accuracy: 98.05%\n",
      "Epoch 2/50, Train Loss: 5.3175, Validation Loss: 0.0596, Validation Accuracy: 98.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/50: 100%|██████████| 183/183 [00:01<00:00, 113.52it/s]\n",
      "Validation Epoch 3: 100%|██████████| 31/31 [00:00<00:00, 135.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Validation Loss: 0.0596, Accuracy: 98.05%\n",
      "Epoch 3/50, Train Loss: 5.3237, Validation Loss: 0.0596, Validation Accuracy: 98.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/50: 100%|██████████| 183/183 [00:01<00:00, 120.71it/s]\n",
      "Validation Epoch 4: 100%|██████████| 31/31 [00:00<00:00, 138.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Validation Loss: 0.0596, Accuracy: 98.05%\n",
      "Epoch 4/50, Train Loss: 5.3211, Validation Loss: 0.0596, Validation Accuracy: 98.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/50: 100%|██████████| 183/183 [00:01<00:00, 119.82it/s]\n",
      "Validation Epoch 5: 100%|██████████| 31/31 [00:00<00:00, 137.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Validation Loss: 0.0596, Accuracy: 98.05%\n",
      "Epoch 5/50, Train Loss: 5.3171, Validation Loss: 0.0596, Validation Accuracy: 98.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/50: 100%|██████████| 183/183 [00:01<00:00, 121.32it/s]\n",
      "Validation Epoch 6: 100%|██████████| 31/31 [00:00<00:00, 131.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Validation Loss: 0.0596, Accuracy: 98.05%\n",
      "Epoch 6/50, Train Loss: 5.3196, Validation Loss: 0.0596, Validation Accuracy: 98.05%\n",
      "Early stopping triggered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(mnist_lora_train_loader, mnist_lora_validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50db149",
   "metadata": {},
   "source": [
    "#### Verify fine-tuning didn't alter original weights but only the ones introduced by LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f8e6d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert torch.all(model.fc1.parametrizations.weight.original == original_weights['fc1.weight'])\n",
    "# assert torch.all(model.fc2.parametrizations.weight.original == original_weights['fc2.weight'])\n",
    "\n",
    "enable_disable_lora(enabled=True)\n",
    "assert torch.equal(model.fc1.weight, model.fc1.parametrizations.weight.original + (model.fc1.parametrizations.weight[0].lora_B @ model.fc1.parametrizations.weight[0].lora_A) * model.fc1.parametrizations.weight[0].scale)\n",
    "enable_disable_lora(enabled=False)\n",
    "assert torch.equal(model.fc1.weight, model.fc1.parametrizations.weight.original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6026d20",
   "metadata": {},
   "source": [
    "#### Test with LoRA enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8d5d2e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 313/313 [00:02<00:00, 136.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0319, Accuracy: 98.92%\n",
      "Digit 0: 99.59% Incorrect: 4 Total: 980\n",
      "Digit 1: 99.65% Incorrect: 4 Total: 1135\n",
      "Digit 2: 99.42% Incorrect: 6 Total: 1032\n",
      "Digit 3: 99.70% Incorrect: 3 Total: 1010\n",
      "Digit 4: 98.57% Incorrect: 14 Total: 982\n",
      "Digit 5: 98.32% Incorrect: 15 Total: 892\n",
      "Digit 6: 98.64% Incorrect: 13 Total: 958\n",
      "Digit 7: 98.93% Incorrect: 11 Total: 1028\n",
      "Digit 8: 98.05% Incorrect: 19 Total: 974\n",
      "Digit 9: 98.12% Incorrect: 19 Total: 1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.031945871058609375, 98.92)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enable_disable_lora(enabled=True)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2a8d1c",
   "metadata": {},
   "source": [
    "#### Test network with LoRA disabled (accuracy and error counts must be the same as the original network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4470771a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 313/313 [00:02<00:00, 150.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0319, Accuracy: 98.92%\n",
      "Digit 0: 99.59% Incorrect: 4 Total: 980\n",
      "Digit 1: 99.65% Incorrect: 4 Total: 1135\n",
      "Digit 2: 99.42% Incorrect: 6 Total: 1032\n",
      "Digit 3: 99.70% Incorrect: 3 Total: 1010\n",
      "Digit 4: 98.57% Incorrect: 14 Total: 982\n",
      "Digit 5: 98.32% Incorrect: 15 Total: 892\n",
      "Digit 6: 98.64% Incorrect: 13 Total: 958\n",
      "Digit 7: 98.93% Incorrect: 11 Total: 1028\n",
      "Digit 8: 98.05% Incorrect: 19 Total: 974\n",
      "Digit 9: 98.12% Incorrect: 19 Total: 1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.031945871058609375, 98.92)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enable_disable_lora(enabled=False)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85200440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
